{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torchsummary import summary\n",
    "\n",
    "from doggofier.data.dataset import DogsDataset\n",
    "from doggofier.models import ResNet50, VGG16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = DogsDataset.create_categories('data')\n",
    "num_classes = len(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet-50\n",
    "\n",
    "ResNet-50 is a residual network made of 50 layers proposed in the paper \"Deep Residual Learning for Image Recognition\". It's widely used for many computer vision tasks because it allows to build very deep networks without hitting vanishing/exploding gradients problem. This is achieved by introducing residual blocks consisting of shortcuts that skip one or more layers. The shortcut connections simply perform identity mapping, and their outputs are added to the outputs of stacked layers. This kind of mapping doesn't add any extra parameter nor computational complexity. In the picture below there is an architecture of ResNet-50 network:\n",
    "\n",
    "![resnet](https://tech.showmax.com/2019/04/conveiro/resnet_50-f66170f4.png)\n",
    "\n",
    "In our case we have removed original classifier and in its place we have added a sequential block consisting of 2 fully connected layers with ReLU as an activation function followed by 2 dropout layers. At the end the fully connected layer with log softmax function has been added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [32, 64, 112, 112]             128\n",
      "              ReLU-3         [32, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [32, 64, 56, 56]               0\n",
      "            Conv2d-5           [32, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [32, 64, 56, 56]             128\n",
      "              ReLU-7           [32, 64, 56, 56]               0\n",
      "            Conv2d-8           [32, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [32, 64, 56, 56]             128\n",
      "             ReLU-10           [32, 64, 56, 56]               0\n",
      "           Conv2d-11          [32, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [32, 256, 56, 56]             512\n",
      "           Conv2d-13          [32, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [32, 256, 56, 56]             512\n",
      "             ReLU-15          [32, 256, 56, 56]               0\n",
      "       Bottleneck-16          [32, 256, 56, 56]               0\n",
      "           Conv2d-17           [32, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [32, 64, 56, 56]             128\n",
      "             ReLU-19           [32, 64, 56, 56]               0\n",
      "           Conv2d-20           [32, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [32, 64, 56, 56]             128\n",
      "             ReLU-22           [32, 64, 56, 56]               0\n",
      "           Conv2d-23          [32, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [32, 256, 56, 56]             512\n",
      "             ReLU-25          [32, 256, 56, 56]               0\n",
      "       Bottleneck-26          [32, 256, 56, 56]               0\n",
      "           Conv2d-27           [32, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [32, 64, 56, 56]             128\n",
      "             ReLU-29           [32, 64, 56, 56]               0\n",
      "           Conv2d-30           [32, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [32, 64, 56, 56]             128\n",
      "             ReLU-32           [32, 64, 56, 56]               0\n",
      "           Conv2d-33          [32, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [32, 256, 56, 56]             512\n",
      "             ReLU-35          [32, 256, 56, 56]               0\n",
      "       Bottleneck-36          [32, 256, 56, 56]               0\n",
      "           Conv2d-37          [32, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [32, 128, 56, 56]             256\n",
      "             ReLU-39          [32, 128, 56, 56]               0\n",
      "           Conv2d-40          [32, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [32, 128, 28, 28]             256\n",
      "             ReLU-42          [32, 128, 28, 28]               0\n",
      "           Conv2d-43          [32, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [32, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [32, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [32, 512, 28, 28]           1,024\n",
      "             ReLU-47          [32, 512, 28, 28]               0\n",
      "       Bottleneck-48          [32, 512, 28, 28]               0\n",
      "           Conv2d-49          [32, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [32, 128, 28, 28]             256\n",
      "             ReLU-51          [32, 128, 28, 28]               0\n",
      "           Conv2d-52          [32, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [32, 128, 28, 28]             256\n",
      "             ReLU-54          [32, 128, 28, 28]               0\n",
      "           Conv2d-55          [32, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [32, 512, 28, 28]           1,024\n",
      "             ReLU-57          [32, 512, 28, 28]               0\n",
      "       Bottleneck-58          [32, 512, 28, 28]               0\n",
      "           Conv2d-59          [32, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [32, 128, 28, 28]             256\n",
      "             ReLU-61          [32, 128, 28, 28]               0\n",
      "           Conv2d-62          [32, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [32, 128, 28, 28]             256\n",
      "             ReLU-64          [32, 128, 28, 28]               0\n",
      "           Conv2d-65          [32, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [32, 512, 28, 28]           1,024\n",
      "             ReLU-67          [32, 512, 28, 28]               0\n",
      "       Bottleneck-68          [32, 512, 28, 28]               0\n",
      "           Conv2d-69          [32, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [32, 128, 28, 28]             256\n",
      "             ReLU-71          [32, 128, 28, 28]               0\n",
      "           Conv2d-72          [32, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [32, 128, 28, 28]             256\n",
      "             ReLU-74          [32, 128, 28, 28]               0\n",
      "           Conv2d-75          [32, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [32, 512, 28, 28]           1,024\n",
      "             ReLU-77          [32, 512, 28, 28]               0\n",
      "       Bottleneck-78          [32, 512, 28, 28]               0\n",
      "           Conv2d-79          [32, 256, 28, 28]         131,072\n",
      "      BatchNorm2d-80          [32, 256, 28, 28]             512\n",
      "             ReLU-81          [32, 256, 28, 28]               0\n",
      "           Conv2d-82          [32, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-83          [32, 256, 14, 14]             512\n",
      "             ReLU-84          [32, 256, 14, 14]               0\n",
      "           Conv2d-85         [32, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-86         [32, 1024, 14, 14]           2,048\n",
      "           Conv2d-87         [32, 1024, 14, 14]         524,288\n",
      "      BatchNorm2d-88         [32, 1024, 14, 14]           2,048\n",
      "             ReLU-89         [32, 1024, 14, 14]               0\n",
      "       Bottleneck-90         [32, 1024, 14, 14]               0\n",
      "           Conv2d-91          [32, 256, 14, 14]         262,144\n",
      "      BatchNorm2d-92          [32, 256, 14, 14]             512\n",
      "             ReLU-93          [32, 256, 14, 14]               0\n",
      "           Conv2d-94          [32, 256, 14, 14]         589,824\n",
      "      BatchNorm2d-95          [32, 256, 14, 14]             512\n",
      "             ReLU-96          [32, 256, 14, 14]               0\n",
      "           Conv2d-97         [32, 1024, 14, 14]         262,144\n",
      "      BatchNorm2d-98         [32, 1024, 14, 14]           2,048\n",
      "             ReLU-99         [32, 1024, 14, 14]               0\n",
      "      Bottleneck-100         [32, 1024, 14, 14]               0\n",
      "          Conv2d-101          [32, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-102          [32, 256, 14, 14]             512\n",
      "            ReLU-103          [32, 256, 14, 14]               0\n",
      "          Conv2d-104          [32, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-105          [32, 256, 14, 14]             512\n",
      "            ReLU-106          [32, 256, 14, 14]               0\n",
      "          Conv2d-107         [32, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-108         [32, 1024, 14, 14]           2,048\n",
      "            ReLU-109         [32, 1024, 14, 14]               0\n",
      "      Bottleneck-110         [32, 1024, 14, 14]               0\n",
      "          Conv2d-111          [32, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-112          [32, 256, 14, 14]             512\n",
      "            ReLU-113          [32, 256, 14, 14]               0\n",
      "          Conv2d-114          [32, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-115          [32, 256, 14, 14]             512\n",
      "            ReLU-116          [32, 256, 14, 14]               0\n",
      "          Conv2d-117         [32, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-118         [32, 1024, 14, 14]           2,048\n",
      "            ReLU-119         [32, 1024, 14, 14]               0\n",
      "      Bottleneck-120         [32, 1024, 14, 14]               0\n",
      "          Conv2d-121          [32, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-122          [32, 256, 14, 14]             512\n",
      "            ReLU-123          [32, 256, 14, 14]               0\n",
      "          Conv2d-124          [32, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-125          [32, 256, 14, 14]             512\n",
      "            ReLU-126          [32, 256, 14, 14]               0\n",
      "          Conv2d-127         [32, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-128         [32, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [32, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [32, 1024, 14, 14]               0\n",
      "          Conv2d-131          [32, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [32, 256, 14, 14]             512\n",
      "            ReLU-133          [32, 256, 14, 14]               0\n",
      "          Conv2d-134          [32, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [32, 256, 14, 14]             512\n",
      "            ReLU-136          [32, 256, 14, 14]               0\n",
      "          Conv2d-137         [32, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [32, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [32, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [32, 1024, 14, 14]               0\n",
      "          Conv2d-141          [32, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-142          [32, 512, 14, 14]           1,024\n",
      "            ReLU-143          [32, 512, 14, 14]               0\n",
      "          Conv2d-144            [32, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-145            [32, 512, 7, 7]           1,024\n",
      "            ReLU-146            [32, 512, 7, 7]               0\n",
      "          Conv2d-147           [32, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-148           [32, 2048, 7, 7]           4,096\n",
      "          Conv2d-149           [32, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-150           [32, 2048, 7, 7]           4,096\n",
      "            ReLU-151           [32, 2048, 7, 7]               0\n",
      "      Bottleneck-152           [32, 2048, 7, 7]               0\n",
      "          Conv2d-153            [32, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-154            [32, 512, 7, 7]           1,024\n",
      "            ReLU-155            [32, 512, 7, 7]               0\n",
      "          Conv2d-156            [32, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-157            [32, 512, 7, 7]           1,024\n",
      "            ReLU-158            [32, 512, 7, 7]               0\n",
      "          Conv2d-159           [32, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-160           [32, 2048, 7, 7]           4,096\n",
      "            ReLU-161           [32, 2048, 7, 7]               0\n",
      "      Bottleneck-162           [32, 2048, 7, 7]               0\n",
      "          Conv2d-163            [32, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-164            [32, 512, 7, 7]           1,024\n",
      "            ReLU-165            [32, 512, 7, 7]               0\n",
      "          Conv2d-166            [32, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-167            [32, 512, 7, 7]           1,024\n",
      "            ReLU-168            [32, 512, 7, 7]               0\n",
      "          Conv2d-169           [32, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-170           [32, 2048, 7, 7]           4,096\n",
      "            ReLU-171           [32, 2048, 7, 7]               0\n",
      "      Bottleneck-172           [32, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-173           [32, 2048, 1, 1]               0\n",
      "          Linear-174                 [32, 2048]       4,196,352\n",
      "            ReLU-175                 [32, 2048]               0\n",
      "         Dropout-176                 [32, 2048]               0\n",
      "          Linear-177                  [32, 256]         524,544\n",
      "            ReLU-178                  [32, 256]               0\n",
      "         Dropout-179                  [32, 256]               0\n",
      "          Linear-180                  [32, 130]          33,410\n",
      "      LogSoftmax-181                  [32, 130]               0\n",
      "          ResNet-182                  [32, 130]               0\n",
      "================================================================\n",
      "Total params: 28,262,338\n",
      "Trainable params: 4,754,306\n",
      "Non-trainable params: 23,508,032\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 9171.41\n",
      "Params size (MB): 107.81\n",
      "Estimated Total Size (MB): 9297.59\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "resnet = ResNet50(num_classes, pretrained=True)\n",
    "\n",
    "summary(resnet, input_size=(3, 224, 224), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16\n",
    "\n",
    "VGG-16 is a convolutional neural network architecture proposed in the paper \"Very Deep Convolutional Networks For Large-Scale Image Recognition\". The most unique thing about this network in comparison to the earlier architectures is the usage of multiple kernel-sized 3x3 filters one after another instead of using large 7x7 or 11x11 filters at the beginning of the network. The convolution stride is fixed to 1 pixel and the spatial padding of convolutional layer input is such that the spatial resolution is preserved after convolution. The architecture is depicted in the picture below:\n",
    "\n",
    "![vgg](https://neurohive.io/wp-content/uploads/2018/11/vgg16-neural-network.jpg)\n",
    "\n",
    "Similarly to ResNet-50 model, again we have replaced the last layer of classifier with one fully connected layer with ReLU function and dropout layer and one fully connected layer with log softmax function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [32, 64, 224, 224]           1,792\n",
      "              ReLU-2         [32, 64, 224, 224]               0\n",
      "            Conv2d-3         [32, 64, 224, 224]          36,928\n",
      "              ReLU-4         [32, 64, 224, 224]               0\n",
      "         MaxPool2d-5         [32, 64, 112, 112]               0\n",
      "            Conv2d-6        [32, 128, 112, 112]          73,856\n",
      "              ReLU-7        [32, 128, 112, 112]               0\n",
      "            Conv2d-8        [32, 128, 112, 112]         147,584\n",
      "              ReLU-9        [32, 128, 112, 112]               0\n",
      "        MaxPool2d-10          [32, 128, 56, 56]               0\n",
      "           Conv2d-11          [32, 256, 56, 56]         295,168\n",
      "             ReLU-12          [32, 256, 56, 56]               0\n",
      "           Conv2d-13          [32, 256, 56, 56]         590,080\n",
      "             ReLU-14          [32, 256, 56, 56]               0\n",
      "           Conv2d-15          [32, 256, 56, 56]         590,080\n",
      "             ReLU-16          [32, 256, 56, 56]               0\n",
      "        MaxPool2d-17          [32, 256, 28, 28]               0\n",
      "           Conv2d-18          [32, 512, 28, 28]       1,180,160\n",
      "             ReLU-19          [32, 512, 28, 28]               0\n",
      "           Conv2d-20          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-21          [32, 512, 28, 28]               0\n",
      "           Conv2d-22          [32, 512, 28, 28]       2,359,808\n",
      "             ReLU-23          [32, 512, 28, 28]               0\n",
      "        MaxPool2d-24          [32, 512, 14, 14]               0\n",
      "           Conv2d-25          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-26          [32, 512, 14, 14]               0\n",
      "           Conv2d-27          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-28          [32, 512, 14, 14]               0\n",
      "           Conv2d-29          [32, 512, 14, 14]       2,359,808\n",
      "             ReLU-30          [32, 512, 14, 14]               0\n",
      "        MaxPool2d-31            [32, 512, 7, 7]               0\n",
      "AdaptiveAvgPool2d-32            [32, 512, 7, 7]               0\n",
      "           Linear-33                 [32, 4096]     102,764,544\n",
      "             ReLU-34                 [32, 4096]               0\n",
      "          Dropout-35                 [32, 4096]               0\n",
      "           Linear-36                 [32, 4096]      16,781,312\n",
      "             ReLU-37                 [32, 4096]               0\n",
      "          Dropout-38                 [32, 4096]               0\n",
      "           Linear-39                  [32, 256]       1,048,832\n",
      "             ReLU-40                  [32, 256]               0\n",
      "          Dropout-41                  [32, 256]               0\n",
      "           Linear-42                  [32, 130]          33,410\n",
      "       LogSoftmax-43                  [32, 130]               0\n",
      "              VGG-44                  [32, 130]               0\n",
      "================================================================\n",
      "Total params: 135,342,786\n",
      "Trainable params: 1,082,242\n",
      "Non-trainable params: 134,260,544\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 18.38\n",
      "Forward/backward pass size (MB): 7001.03\n",
      "Params size (MB): 516.29\n",
      "Estimated Total Size (MB): 7535.70\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vgg = VGG16(num_classes, pretrained=True)\n",
    "\n",
    "summary(vgg, input_size=(3, 224, 224), batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of training\n",
    "\n",
    "We have prepared a script that allows an user to run the training without modifying any code. The only thing that must be done is preparing the JSON file with hyperparameters. The example of such a file is given below:\n",
    "\n",
    "```\n",
    "{\n",
    "    \"n_classes\": 130,\n",
    "    \"num_workers\": 0,\n",
    "    \"batch_size\": 32,\n",
    "    \"lr\": 0.001,\n",
    "    \"epochs\": 1,\n",
    "    \"max_epoch_stop\": 3\n",
    "}\n",
    "```\n",
    "It is assumed that the JSON file is stored in `models` directory and dataset is stored in `data` directory, however user can change it by specifying appropriate script arguments. Here's presented only exemplary training of VGG-16 model for 1 epoch with the file given above. The proper training will be performed on device with GPUs to speed up computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading the datasets...\n",
      "Dataset loading has been completed.\n",
      "Training on cpu device...\n",
      "Epoch: 1 / 1\t                    Step: 100 / 2039\t                    Loss: 2.6234065544605256\n",
      "Epoch: 1 / 1\t                    Step: 200 / 2039\t                    Loss: 2.2220068752765654\n",
      "Epoch: 1 / 1\t                    Step: 300 / 2039\t                    Loss: 1.9844504523277282\n",
      "Epoch: 1 / 1\t                    Step: 400 / 2039\t                    Loss: 1.8420568689703942\n",
      "Epoch: 1 / 1\t                    Step: 500 / 2039\t                    Loss: 1.7455630342960358\n",
      "Epoch: 1 / 1\t                    Step: 600 / 2039\t                    Loss: 1.683325431148211\n",
      "Epoch: 1 / 1\t                    Step: 700 / 2039\t                    Loss: 1.6278196588584355\n",
      "Epoch: 1 / 1\t                    Step: 800 / 2039\t                    Loss: 1.5850052423030139\n",
      "Epoch: 1 / 1\t                    Step: 900 / 2039\t                    Loss: 1.5520649543073441\n",
      "Epoch: 1 / 1\t                    Step: 1000 / 2039\t                    Loss: 1.5240974884033203\n",
      "Epoch: 1 / 1\t                    Step: 1100 / 2039\t                    Loss: 1.49614803601395\n",
      "Epoch: 1 / 1\t                    Step: 1200 / 2039\t                    Loss: 1.4748568344612916\n",
      "Epoch: 1 / 1\t                    Step: 1300 / 2039\t                    Loss: 1.4558660620450974\n",
      "Epoch: 1 / 1\t                    Step: 1400 / 2039\t                    Loss: 1.4421633599911416\n",
      "Epoch: 1 / 1\t                    Step: 1500 / 2039\t                    Loss: 1.4309318575461705\n",
      "Epoch: 1 / 1\t                    Step: 1600 / 2039\t                    Loss: 1.4206649467349053\n",
      "Epoch: 1 / 1\t                    Step: 1700 / 2039\t                    Loss: 1.4115951699018479\n",
      "Epoch: 1 / 1\t                    Step: 1800 / 2039\t                    Loss: 1.4034987480110592\n",
      "Epoch: 1 / 1\t                    Step: 1900 / 2039\t                    Loss: 1.3969750675715897\n",
      "Epoch: 1 / 1\t                    Step: 2000 / 2039\t                    Loss: 1.388633533835411\n",
      "Epoch 1 has ended!\n",
      "            Train loss: 1.3854218485777492\t            Validation loss: 0.9259641185039427\t            Accuracy: 0.7192307692307692\n",
      "Training has been completed.\n"
     ]
    }
   ],
   "source": [
    "%run doggofier/train.py vgg16_example.json vgg16"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
